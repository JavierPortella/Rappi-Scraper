{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cf7df4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías a usar\n",
    "from concurrent import futures as futures\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from traceback import TracebackException\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "from openpyxl import load_workbook, Workbook\n",
    "import pandas as pd\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from seleniumwire.utils import decode\n",
    "from seleniumwire.webdriver import Chrome, ChromeOptions, Firefox, FirefoxOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "\n",
    "# Constantes\n",
    "CURRENT_DATE = datetime.now().date()\n",
    "ROOT_PATH = os.getcwd()\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "THREAD = futures.ThreadPoolExecutor()\n",
    "ENV = dotenv_values()\n",
    "DATA_FILENAME = ENV[\"DATA_FILENAME\"]\n",
    "DATA_FOLDER = ENV[\"DATA_FOLDER\"]\n",
    "LOG_FILENAME = ENV[\"LOG_FILENAME\"]\n",
    "LOG_FOLDER = ENV[\"LOG_FOLDER\"]\n",
    "METADATA_FILENAME = ENV[\"METADATA_FILENAME\"]\n",
    "METADATA_SHEET_NAME = ENV[\"METADATA_SHEET_NAME\"]\n",
    "FB_USERNAME = ENV[\"FB_USERNAME\"]\n",
    "FB_PASSWORD = ENV[\"FB_PASSWORD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a808ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metadata:\n",
    "    \"\"\"Representa a la información generada durante la ejecución del scraper\n",
    "\n",
    "    Attributes:\n",
    "        start_time (float): Hora de inicio de la ejecución del scraper en segundos\n",
    "        end_time (float): Hora de fin de la ejecución del scraper en segundos\n",
    "        execution_date (str): Fecha de extracción de las categorias en formato %d/%m/%Y\n",
    "        start_hour (str): Hora de inicio de la ejecución del scraper en formato %H:%M:%S\n",
    "        end_hour (str): Hora de término de la ejecución del scraper en formato %H:%M:%S\n",
    "        quantity (int): Cantidad de los productos extraídos de la página de Rappi\n",
    "        time_execution (str): Tiempo de ejecución del scraper en segundos\n",
    "        products_per_min (float): Cantidad de categorías que puede extraer el scraper en un minuto\n",
    "        num_errors (int): Cantidad de errores ocurridos durante la ejecución del scraper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Genera todos los atributos para una instancia de la clase Metadata\"\"\"\n",
    "        self._start_time = time.time()\n",
    "        self._end_time = self._start_time\n",
    "        self._execution_date = CURRENT_DATE.strftime(\"%d/%m/%Y\")\n",
    "        self._start_hour = time.strftime(\"%H:%M:%S\", time.localtime(self._start_time))\n",
    "        self._end_hour = self._start_hour\n",
    "        self._quantity = 0\n",
    "        self._time_execution = \"0\"\n",
    "        self._products_per_min = 0\n",
    "        self._num_errors = 0\n",
    "\n",
    "    @property\n",
    "    def num_errors(self):\n",
    "        \"\"\"Retorna el valor actual o actualiza el valor del atributo num_errors\"\"\"\n",
    "        return self._num_errors\n",
    "\n",
    "    @property\n",
    "    def quantity(self):\n",
    "        \"\"\"Retorna el valor actual o actualiza el valor del atributo quantity\"\"\"\n",
    "        return self._quantity\n",
    "\n",
    "    @num_errors.setter\n",
    "    def num_errors(self, num_errors):\n",
    "        self._num_errors = num_errors\n",
    "\n",
    "    @quantity.setter\n",
    "    def quantity(self, quantity):\n",
    "        self._quantity = quantity\n",
    "\n",
    "    def set_attributes_values(self):\n",
    "        \"\"\"Establece los parámetros finales cuando se termina de ejecutar el scraper\"\"\"\n",
    "        self._end_time = time.time()\n",
    "        self._end_hour = time.strftime(\"%H:%M:%S\", time.localtime(self._end_time))\n",
    "        time_execution = self._end_time - self._start_time\n",
    "        if time_execution > 0:\n",
    "            self._time_execution = str(timedelta(seconds=time_execution)).split(\".\")[0]\n",
    "            self._products_per_min = round((self._quantity * 60) / time_execution, 2)\n",
    "\n",
    "    def print_metadata_information(self):\n",
    "        \"\"\"Imprime la información del tiempo de ejecución del scraper por consola\"\"\"\n",
    "        LOGGER.info(f\"Hora inicio: {self._start_hour}\")\n",
    "        LOGGER.info(f\"Hora Fin: {self._end_hour}\")\n",
    "        LOGGER.info(f\"Duración: {self._time_execution}\")\n",
    "        LOGGER.info(f\"Productos Extraídos: {self._quantity}\")\n",
    "        LOGGER.info(f\"Productos Extraídos / min: {self._products_per_min}\")\n",
    "        LOGGER.info(f\"Número de errores: {self._num_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4f9fc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(TracebackException):\n",
    "    \"\"\"Extiende la clase TracebackException para el manejo del traceback\"\"\"\n",
    "\n",
    "    def __init__(self, error) -> None:\n",
    "        \"\"\"Genera todos los atributos para una instancia de la clase Error\n",
    "\n",
    "        Args:\n",
    "            error (Exception): Error ocurrido durante la ejecución del scraper\n",
    "        \"\"\"\n",
    "        super().__init__(type(error), error, error.__traceback__)\n",
    "\n",
    "    def print_error_detail(self):\n",
    "        \"\"\"Imprime toda la información del error por consola\"\"\"\n",
    "        LOGGER.error(\"Ha ocurrido un error:\")\n",
    "        for line in self.format(chain=True):\n",
    "            LOGGER.error(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4bb2f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperRappiProducts:\n",
    "    \"\"\"Representa a un bot para hacer web scraping en Rappi\n",
    "\n",
    "    Attributes:\n",
    "        metadata (Metadata): Objeto que maneja toda la información generada por el scraper durante su ejecución\n",
    "        products (list): Lista que contiene todos los productos que ofertan los restaurantes en Rappi\n",
    "        dataset (DataFrame): DataFrame que contiene toda la información extraída por el scraper\n",
    "        links_to_go (list): Lista de restaurantes que en un primer intento no se pudo extraer su información\n",
    "        driver (WebDriver): Objeto que maneja el navegador web\n",
    "        wait (WebDriverWait): Objeto que maneja los tiempos de espera de búsqueda de elementos en la web\n",
    "        action (ActionChains): Objeto que maneja las acciones que se pueden aplicar a los elementos en la web\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Genera todos los atributos para una instancia de la clase ScraperRappiProducts\"\"\"\n",
    "        LOGGER.info(\"Inicializando scraper\")\n",
    "        self._metadata = Metadata()\n",
    "        self._products = []\n",
    "        self._restaurants = []\n",
    "        self._dataset = pd.DataFrame()\n",
    "        self._links_to_go = []\n",
    "        firefox_options = FirefoxOptions()\n",
    "        self._driver = Firefox(options=firefox_options,service=Service(GeckoDriverManager().install()))\n",
    "        self._driver.maximize_window()\n",
    "        self._wait = WebDriverWait(self._driver, 10)\n",
    "        self._action = ActionChains(self._driver)\n",
    "        LOGGER.info(\"Scraper inicializado satisfactoriamente\")\n",
    "\n",
    "    def login(self, user_name, user_password):\n",
    "        \"\"\"Inicia sesión en la página web de Rapi usando una cuenta de Facebook\n",
    "\n",
    "        Args:\n",
    "            user_name (str): Usuario activo de facebook\n",
    "            user_password (str): Contraseña del usuario activo de facebook\n",
    "        \"\"\"\n",
    "        LOGGER.info(\"Iniciando sesión\")\n",
    "        self._driver.get(\"https://www.rappi.com.pe/login\")\n",
    "        # Usar la opción de facebook\n",
    "        self._driver.find_element(\n",
    "            By.XPATH, \"//button[@class='chakra-button css-1hdh3ss']\"\n",
    "        ).click()\n",
    "        time.sleep(random.uniform(3.0, 4.0))\n",
    "        # Cambiar a la pestaña de inicio de sesión de Facebook\n",
    "        self._driver.switch_to.window(self._driver.window_handles[1])\n",
    "        # Completar los campos de usuario y contraseña\n",
    "        username = self._wait.until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "        password = self._wait.until(EC.presence_of_element_located((By.ID, \"pass\")))\n",
    "        username.clear()\n",
    "        password.clear()\n",
    "        username.send_keys(user_name)\n",
    "        password.send_keys(user_password)\n",
    "        # Iniciar sesión\n",
    "        self._driver.find_element(By.CSS_SELECTOR, \"input[name='login']\").click()\n",
    "        # Dando permisos a Rappi en el primer inicio de sesión\n",
    "        try:\n",
    "            self._wait.until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (\n",
    "                        By.XPATH,\n",
    "                        \"//div[@class='x1r8uery x1iyjqo2']\",\n",
    "                    )\n",
    "                )\n",
    "            ).click()\n",
    "        except:\n",
    "            pass\n",
    "        # Volver a la pestaña principal\n",
    "        self._driver.switch_to.window(self._driver.window_handles[0])\n",
    "        time.sleep(random.uniform(8.0, 10.0))\n",
    "        # Detectar si el ícono del usuario aparece en la página\n",
    "        self._wait.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.XPATH, \"//div[@class='sc-fdt2fy-11 dJNVzE']\")\n",
    "            )\n",
    "        )\n",
    "        LOGGER.info(\"Se inició sesión correctamente\")\n",
    "\n",
    "    def scrap_product(self, product, rest, status, cat):\n",
    "        \"\"\"Extrae la información de un producto de un restaurante\n",
    "\n",
    "        Args:\n",
    "            product (WebElement): Elemento web que contiene la información del producto de un restaurante\n",
    "            rest (str): Nombre Restaurante\n",
    "            status (str): Estado del restaurante\n",
    "            cat (str): Categoría del restaurante\n",
    "\n",
    "        Returns:\n",
    "            list: Arreglo con toda la información del producto de un restaurante\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        try:\n",
    "            # Popularidad\n",
    "            try:\n",
    "                product.find_element(\n",
    "                    By.XPATH, \".//p[@class='chakra-text css-n0gvg7']\"\n",
    "                ).text\n",
    "                data.append(True)\n",
    "            except:\n",
    "                data.append(False)\n",
    "            # Nombre\n",
    "            data.append(\n",
    "                product.find_element(By.XPATH, \".//div[@class='css-k008qs']\").text\n",
    "            )\n",
    "            # Descripción\n",
    "            data.append(\n",
    "                product.find_element(\n",
    "                    By.XPATH,\n",
    "                    \".//p[@class='chakra-text sc-a04fe063-2 gHQcCO css-1rmjo0r']\",\n",
    "                ).text\n",
    "            )\n",
    "            # Precios\n",
    "            prices = product.find_element(\n",
    "                By.XPATH, \".//div[contains(@class, 'chakra-skeleton')]\"\n",
    "            ).text.split(\"S/ \")\n",
    "            # Precio con descuento\n",
    "            try:\n",
    "                data.append(prices[-2])\n",
    "            except:\n",
    "                data.append(None)\n",
    "            # Precio sin descuento\n",
    "            data.append(prices[-1])\n",
    "            # Nombre del restaurante\n",
    "            data.append(rest)\n",
    "            # Disponibilidad\n",
    "            data.append(status)\n",
    "            # Categoría\n",
    "            data.append(cat)\n",
    "            return data\n",
    "        except Exception as error:\n",
    "            try:\n",
    "                LOGGER.error(f\"Error al scrapear el producto {data[1]}\")\n",
    "            except:\n",
    "                pass\n",
    "            LOGGER.error(f\"Error de tipo: {error.__class__}\")\n",
    "            return []\n",
    "\n",
    "    def scrap_restaurante(self, rest, link, status):\n",
    "        \"\"\"Ingresa a un restaurante y extrae la información de todos los productos que ofrece\n",
    "\n",
    "        Args:\n",
    "            rest (str): Nombre del restaurante\n",
    "            link (str): Link del restaurante\n",
    "            status (str): Estado actual del restaurante\n",
    "        \"\"\"\n",
    "        try:\n",
    "            LOGGER.info(f\"Extrayendo información del restaurante {rest}\")\n",
    "            self._driver.get(link)\n",
    "            # Productos que ofrece el restaurante\n",
    "            products = self._driver.find_elements(\n",
    "                By.XPATH, '//div[@class=\"chakra-stack css-46p1lt\"]'\n",
    "            )\n",
    "            # Categoría del restaurante\n",
    "            try:\n",
    "                category = self._driver.find_element(\n",
    "                    By.XPATH, \"//div[@class='sc-3627ee44-1 dEYlRK']/h2[2]\"\n",
    "                ).text\n",
    "            except:\n",
    "                LOGGER.info(f\"El restaurante no posee ninguna categoría\")\n",
    "                category = None\n",
    "            # Extrayendo la información de varios productos a la vez\n",
    "            future_products = [\n",
    "                THREAD.submit(self.scrap_product, product, rest, status, category)\n",
    "                for product in products\n",
    "            ]\n",
    "            for future_product in futures.wait(future_products).done:\n",
    "                self._products.append(future_product.result())\n",
    "        except Exception as error:\n",
    "            LOGGER.error(\"Fallo al extraer la información de los productos\")\n",
    "            LOGGER.error(f\"Error de tipo: {error.__class__}\")\n",
    "\n",
    "    def extract_products(self, restaurant):\n",
    "        \"\"\"Extrae la información de los productos de un restaurante usando los recursos de red\n",
    "\n",
    "        Args:\n",
    "            restaurant (WebElement): Elemento web que representa a un restaurante\n",
    "        \"\"\"\n",
    "        try:\n",
    "            try:\n",
    "                restaurant_status = restaurant.find_element(\n",
    "                    By.XPATH, \".//p[contains(@class,'chakra-text')]\"\n",
    "                ).text\n",
    "            except:\n",
    "                restaurant_status = \"Restaurante abierto\"\n",
    "            # Enlace web del restaurante\n",
    "            restaurant_link = restaurant.get_attribute(\"href\")\n",
    "            self._restaurants.append(restaurant_link)\n",
    "            # Request que contiene la información del restaurante\n",
    "            request = self._driver.wait_for_request(\n",
    "                re.search(\"restaurantes/(.*)\", restaurant_link).group(1),\n",
    "                timeout=3,\n",
    "            )\n",
    "            # Obteniendo la respuesta decodificada del request\n",
    "            decoded_body = decode(\n",
    "                request.response.body,\n",
    "                request.response.headers.get(\"Content-Encoding\", \"identity\"),\n",
    "            ).decode(\"utf-8\")\n",
    "            # Convirtiendo en formato json\n",
    "            json_data = json.loads(decoded_body)\n",
    "            rest_dict = json_data[\"pageProps\"][\"fallback\"]\n",
    "            restaurant_data = rest_dict[next(iter(rest_dict))]\n",
    "            # Nombre y categoría del restaurante\n",
    "            restaurant_name = restaurant_data.get(\"brandName\")\n",
    "            restaurant_category = restaurant_data.get(\"categories\")\n",
    "            return [\n",
    "                [\n",
    "                    product.get(\"isPopular\", False),\n",
    "                    product.get(\"name\"),\n",
    "                    product.get(\"description\"),\n",
    "                    product.get(\"priceNumber\"),\n",
    "                    product.get(\"realPrice\"),\n",
    "                    restaurant_name,\n",
    "                    restaurant_status,\n",
    "                    restaurant_category,\n",
    "                ]\n",
    "                for product_category in restaurant_data[\"corridors\"]\n",
    "                for product in product_category[\"products\"]\n",
    "            ]\n",
    "        except Exception as error:\n",
    "            self._metadata.num_errors += 1\n",
    "            rest_name = restaurant.get_attribute(\"aria-label\")\n",
    "            LOGGER.error(\n",
    "                f\"Fallo al intentar extraer la información de los productos ofrecidos por el restaurante {rest_name}\"\n",
    "            )\n",
    "            LOGGER.error(error)\n",
    "            self._links_to_go.append((rest_name, restaurant_link, restaurant_status))\n",
    "            return []\n",
    "\n",
    "    def extract_data(self):\n",
    "        \"\"\"Extrae todos los productos que ofertan los restaurantes en Rappi\"\"\"\n",
    "        LOGGER.info(\"Extrayendo los productos de rappi\")\n",
    "        self._driver.get(\"https://www.rappi.com.pe/restaurantes\")\n",
    "        del self._driver.requests\n",
    "\n",
    "        # Extracción de información de los restaurantes más cercanos\n",
    "        no_error = True\n",
    "        start = 0\n",
    "        while no_error:\n",
    "            # Identificando el botón de ver más restaurantes\n",
    "            try:\n",
    "                button = self._wait.until(\n",
    "                    EC.element_to_be_clickable(\n",
    "                        (\n",
    "                            By.XPATH,\n",
    "                            \"//button[@class='sc-hqyNC sc-jbKcbu bvSdOe primary wide']\",\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            except Exception as error:\n",
    "                no_error = False\n",
    "\n",
    "            # Esperando a que se carguen los restaurantes\n",
    "            time.sleep(random.uniform(2.0, 3.0))\n",
    "            restaurants = self._wait.until(\n",
    "                lambda x: x.find_elements(\n",
    "                    By.XPATH,\n",
    "                    \"//div[@class='sc-c2b2dc55-4 bkatcD']/a\",\n",
    "                )\n",
    "            )\n",
    "            end = len(restaurants)\n",
    "            for index in range(start, end):\n",
    "                #self._driver.execute_script(\"arguments[0].scrollIntoView(true);\", restaurants[index])\n",
    "                self._action.scroll_to_element(restaurants[index]).move_to_element(restaurants[index]).perform()\n",
    "            # Interactuando con varios restaurantes a la vez\n",
    "            future_restaurants = [\n",
    "                THREAD.submit(self.extract_products, restaurants[index])\n",
    "                for index in range(start, end)\n",
    "            ]\n",
    "            for future_restaurant in futures.wait(future_restaurants).done:\n",
    "                self._products += future_restaurant.result()\n",
    "            LOGGER.info(f\"Cantidad de restaurantes totales recorridos: {end}\")\n",
    "            # Dar click al botón de ver más restaurantes\n",
    "            try:\n",
    "                del self._driver.requests\n",
    "                button.click()\n",
    "                start = end\n",
    "            except:\n",
    "                no_error = False\n",
    "\n",
    "        LOGGER.info(\n",
    "            f\"Se extrajo la información de {len(self._restaurants)} restaurante(s)\"\n",
    "        )\n",
    "\n",
    "        # Extracción de información de los restaurantes por categorías\n",
    "        self._driver.get(\"https://www.rappi.com.pe/restaurantes\")\n",
    "        del self._driver.requests\n",
    "        # Categorías de los restaurantes\n",
    "        categories = self._driver.find_elements(\n",
    "            By.XPATH, \"//button[@class='sc-5d042f5c-1 iyWZJm']\"\n",
    "        )\n",
    "        LOGGER.info(\n",
    "            f\"Se han detectado {len(categories)} tipos de restaurantes según su categoría\"\n",
    "        )\n",
    "        for category in categories:\n",
    "            LOGGER.info(f\"Categoría: {category.text}\")\n",
    "            # Navegar a la parte superior de la página\n",
    "            self._driver.execute_script(\n",
    "                \"window.scrollTo(document.body.scrollHeight, 0)\"\n",
    "            )\n",
    "            time.sleep(random.uniform(1.0, 1.5))\n",
    "            # Dar click a una categoría\n",
    "            try:\n",
    "                category.click()\n",
    "                time.sleep(random.uniform(7.0, 8.0))\n",
    "            except Exception as error:\n",
    "                LOGGER.error(\"No se ha podido dar click a la categoría\")\n",
    "                LOGGER.error(f\"Error de tipo {error.__class__}\")\n",
    "                continue\n",
    "            # Dar click a la flecha de navegación de las categorías\n",
    "            try:\n",
    "                self._driver.find_element(By.CLASS_NAME, \"sc-69ee8a42-2\").click()\n",
    "            except:\n",
    "                pass\n",
    "            # Identificar si la categoría cuenta con restaurantes\n",
    "            try:\n",
    "                # Identificar los restaurantes pertenecientes a la categoría seleccionada\n",
    "                restaurants = self._wait.until(\n",
    "                    lambda x: x.find_elements(\n",
    "                        By.XPATH, \"//div[@class='sc-c2b2dc55-4 bkatcD']/a\"\n",
    "                    )\n",
    "                )\n",
    "                # Filtrar los restaurantes con las que ya se cuente información\n",
    "                restaurants = [\n",
    "                    restaurant\n",
    "                    for restaurant in restaurants\n",
    "                    if restaurant.get_attribute(\"href\") not in self._restaurants\n",
    "                ]\n",
    "                del self._driver.requests\n",
    "                LOGGER.info(\n",
    "                    f\"Se va a extraer información de los productos de {len(restaurants)} restaurante(s)\"\n",
    "                )\n",
    "            except:\n",
    "                LOGGER.info(\"La categoría no cuenta con restaurantes\")\n",
    "                continue\n",
    "            # Extraer la información de varios restaurantes a la vez\n",
    "            future_restaurants = [\n",
    "                THREAD.submit(self.extract_products, restaurant)\n",
    "                for restaurant in restaurants\n",
    "            ]\n",
    "            for future_restaurant in futures.wait(future_restaurants).done:\n",
    "                self._products += future_restaurant.result()\n",
    "            del self._driver.requests\n",
    "\n",
    "        # Eliminar valores duplicados:\n",
    "        self._links_to_go = list(set(self._links_to_go))\n",
    "        LOGGER.info(f\"Se van a recorrer {len(self._links_to_go)} restaurantes\")\n",
    "        # Extrayendo la información de los restaurantes faltantes\n",
    "        for rest_name, restaurant_link, restaurant_status in self._links_to_go:\n",
    "            del self._driver.requests\n",
    "            self.scrap_restaurante(rest_name, restaurant_link, restaurant_status)\n",
    "        LOGGER.info(\"Extracción de datos completada satisfactoriamente\")\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"Proceso de limpieza de datos extraídos por el scraper\"\"\"\n",
    "        try:\n",
    "            LOGGER.info(\"Limpiando la data extraída por el scraper\")\n",
    "            self._dataset = pd.DataFrame(\n",
    "                self._products,\n",
    "                columns=[\n",
    "                    \"Popular\",\n",
    "                    \"Producto\",\n",
    "                    \"Descripcion\",\n",
    "                    \"Precio con descuento\",\n",
    "                    \"Precio sin descuento\",\n",
    "                    \"Restaurante\",\n",
    "                    \"Disponible\",\n",
    "                    \"Categoria\",\n",
    "                ],\n",
    "            )\n",
    "            self._products = []\n",
    "            self._dataset[\"Fecha\"] = CURRENT_DATE.strftime(\"%Y-%m-%d\")\n",
    "            self._dataset.sort_values(\n",
    "                [\"Restaurante\", \"Producto\", \"Descripcion\", \"Popular\"],\n",
    "                inplace=True,\n",
    "                ascending=[True, True, True, False],\n",
    "            )\n",
    "            self._dataset.drop_duplicates(\n",
    "                [\"Restaurante\", \"Producto\", \"Descripcion\"], keep=\"first\", inplace=True\n",
    "            )\n",
    "            self._dataset.replace({\"\": None}, inplace=True)\n",
    "            self._dataset = self._dataset.astype(\n",
    "                {\n",
    "                    \"Precio con descuento\": float,\n",
    "                    \"Precio sin descuento\": float,\n",
    "                    \"Popular\": str,\n",
    "                }\n",
    "            )\n",
    "            self._dataset[\"Precio con descuento\"] = self._dataset[\n",
    "                \"Precio con descuento\"\n",
    "            ].apply(lambda x: round(x, 2))\n",
    "            self._dataset.loc[\n",
    "                self._dataset[\n",
    "                    self._dataset[\"Precio con descuento\"]\n",
    "                    == self._dataset[\"Precio sin descuento\"]\n",
    "                ].index,\n",
    "                \"Precio con descuento\",\n",
    "            ] = None\n",
    "            self._dataset[\n",
    "                [\"Precio con descuento\", \"Precio sin descuento\"]\n",
    "            ] = self._dataset[\n",
    "                [\"Precio con descuento\", \"Precio sin descuento\"]\n",
    "            ].applymap(\n",
    "                \"{:,.2f}\".format\n",
    "            )\n",
    "            self._dataset[[\"Precio con descuento\", \"Precio sin descuento\"]] = (\n",
    "                self._dataset[[\"Precio con descuento\", \"Precio sin descuento\"]]\n",
    "                .replace({\",\": \";\", \"\\.\": \",\"}, regex=True)\n",
    "                .replace({\";\": \".\"}, regex=True)\n",
    "            )\n",
    "            self._dataset[\"Disponible\"].replace(\n",
    "                \"^Abre.+\", \"Restaurante cerrado\", regex=True, inplace=True\n",
    "            )\n",
    "            self._dataset[\"Categoria\"].replace(\" -.+\", \"\", regex=True, inplace=True)\n",
    "            self._dataset[\"Restaurante\"].replace(\" -.+\", \"\", regex=True, inplace=True)\n",
    "            self._dataset[\"Popular\"].replace(\n",
    "                {\"True\": \"popular\", \"False\": \"\"}, inplace=True\n",
    "            )\n",
    "            LOGGER.info(\"Se ha limpiado la data satisfactoriamente\")\n",
    "        except Exception as error:\n",
    "            LOGGER.error(\"Error al ejecutar el proceso completo de limpieza de datos\")\n",
    "            LOGGER.error(error)\n",
    "\n",
    "    def save_data(self, filepath, filename, encoding=\"utf-8-sig\"):\n",
    "        \"\"\"Guarda los datos o errores obtenidos durante la ejecución del scraper\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Ruta del archivo\n",
    "            filename (str): Nombre del archivo\n",
    "            encoding (str): Codificación usada para guardar el archivo. Defaults to \"utf-8-sig\"\n",
    "        \"\"\"\n",
    "        LOGGER.info(\"Guardando la data\")\n",
    "        # Comprobando que el dataset contenga información\n",
    "        if len(self._dataset) == 0:\n",
    "            LOGGER.info(\n",
    "                f\"El archivo de datos no se va a guardar por no tener información\",\n",
    "            )\n",
    "            return\n",
    "\n",
    "        self._metadata.quantity = len(self._dataset)\n",
    "        # Generando la ruta donde se va a guardar la información\n",
    "        filepath = os.path.join(filepath, CURRENT_DATE.strftime(\"%d-%m-%Y\"))\n",
    "        filename = (\n",
    "            filename\n",
    "            + \"_\"\n",
    "            + CURRENT_DATE.strftime(\"%Y-%m-%d\")\n",
    "            + \"_\"\n",
    "            + str(self._metadata.quantity)\n",
    "            + \".csv\"\n",
    "        )\n",
    "\n",
    "        # Verificando si la ruta donde se va a guardar la información existe\n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "\n",
    "        self._dataset.to_csv(\n",
    "            os.path.join(filepath, filename),\n",
    "            sep=\";\",\n",
    "            index=False,\n",
    "            encoding=encoding,\n",
    "        )\n",
    "        LOGGER.info(\n",
    "            f\"El archivo de datos {filename} ha sido guardado correctamente en la ruta {os.path.join(ROOT_PATH, filepath)}\",\n",
    "        )\n",
    "\n",
    "    def save_metadata(self, filename, sheet_name):\n",
    "        \"\"\"Guarda la información de la metadata generada durante la ejecución del scraper\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo\n",
    "            sheet_name (str): Nombre de la hoja de cálculo\n",
    "        \"\"\"\n",
    "        LOGGER.info(\"Guardando la metadata\")\n",
    "        self._metadata.set_attributes_values()\n",
    "        self._metadata.print_metadata_information()\n",
    "        # Variable que indica si el encabezado existe o no en el archivo de excel\n",
    "        header_exist = False\n",
    "\n",
    "        # Verificando si el archivo existe o no\n",
    "        if os.path.isfile(filename):\n",
    "            wb_time = load_workbook(filename)\n",
    "            # Comprobando si ya existe un sheet con el nombre indicado en la variable sheet_name\n",
    "            if sheet_name not in [ws.title for ws in wb_time.worksheets]:\n",
    "                # Creando un nuevo sheet\n",
    "                wb_time.create_sheet(sheet_name)\n",
    "            else:\n",
    "                header_exist = True\n",
    "        else:\n",
    "            wb_time = Workbook()\n",
    "            wb_time.worksheets[0].title = sheet_name\n",
    "\n",
    "        # Seleccionar el sheet deseado donde se va a guardar la información\n",
    "        worksheet = wb_time[sheet_name]\n",
    "\n",
    "        # Comprobando si el encabezado existe o no\n",
    "        if not header_exist:\n",
    "            keys = [\n",
    "                \"Fecha\",\n",
    "                \"Hora Inicio\",\n",
    "                \"Hora Fin\",\n",
    "                \"Cantidad\",\n",
    "                \"Tiempo Ejecucion (min)\",\n",
    "                \"Productos / Minuto\",\n",
    "                \"Errores\",\n",
    "            ]\n",
    "            worksheet.append(keys)\n",
    "\n",
    "        values = list(self._metadata.__dict__.values())[2:]\n",
    "        worksheet.append(values)\n",
    "        wb_time.save(filename)\n",
    "        wb_time.close()\n",
    "        LOGGER.info(\n",
    "            f\"El archivo de la metadata del scraper {filename} ha sido guardado correctamente en la ruta {ROOT_PATH}\",\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Ejecuta el proceso completo de web scraping a Rappi\"\"\"\n",
    "        self.login(FB_USERNAME, FB_PASSWORD)\n",
    "        self.extract_data()\n",
    "        self.process_data()\n",
    "        self.save_data(DATA_FOLDER, DATA_FILENAME)\n",
    "        self.save_metadata(METADATA_FILENAME, METADATA_SHEET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3c70c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_log(\n",
    "    log_folder, log_filename, log_file_mode=\"w\", log_file_encoding=\"utf-8\"\n",
    "):\n",
    "    \"\"\"Función que configura los logs para rastrear al programa\n",
    "\n",
    "    Args:\n",
    "        log_folder (str): Carpeta donde se va a generar el archivo log\n",
    "        log_filename (str): Nombre del archivo log a ser generado\n",
    "        log_file_mode (str, optional): Modo de guardado del archivo. Defaults to \"w\".\n",
    "        log_file_encoding (str, optional): Codificación usada para el archivo. Defaults to \"utf-8\".\n",
    "    \"\"\"\n",
    "    # Generando la ruta donde se va a guardar los registros de ejecución\n",
    "    log_path = os.path.join(log_folder, CURRENT_DATE.strftime(\"%d-%m-%Y\"))\n",
    "    log_filename = log_filename + \"_\" + CURRENT_DATE.strftime(\"%d%m%Y\") + \".log\"\n",
    "\n",
    "    # Verificando si la ruta donde se va a guardar los registros de ejecución existe\n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path)\n",
    "\n",
    "    # Agregando los handlers al logger\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    file_handler = logging.FileHandler(\n",
    "        os.path.join(log_path, log_filename), log_file_mode, log_file_encoding\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "    LOGGER.handlers = [stream_handler, file_handler]\n",
    "    LOGGER.propagate = False\n",
    "    LOGGER.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f4343142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        configure_log(LOG_FOLDER, LOG_FILENAME)\n",
    "        scraper = ScraperRappiProducts()\n",
    "        scraper.run()\n",
    "        LOGGER.info(\"Programa finalizado\")\n",
    "    except Exception as error:\n",
    "        Error(error).print_error_detail()\n",
    "        LOGGER.error(\"Programa ejecutado con fallos\")\n",
    "    finally:\n",
    "        # Liberar el archivo log\n",
    "        logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ebe4da23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 23:30:42,191 - INFO - Inicializando scraper\n",
      "[WDM] - Downloading: 19.2kB [00:00, ?B/s]                                                                              \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[57], line 4\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     configure_log(LOG_FOLDER, LOG_FILENAME)\n\u001b[1;32m----> 4\u001b[0m     scraper \u001b[38;5;241m=\u001b[39m \u001b[43mScraperRappiProducts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     scraper\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m      6\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrograma finalizado\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[55], line 23\u001b[0m, in \u001b[0;36mScraperRappiProducts.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_links_to_go \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m firefox_options \u001b[38;5;241m=\u001b[39m FirefoxOptions()\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver \u001b[38;5;241m=\u001b[39m \u001b[43mFirefox\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirefox_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mService\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGeckoDriverManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver\u001b[38;5;241m.\u001b[39mmaximize_window()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait \u001b[38;5;241m=\u001b[39m WebDriverWait(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver, \u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\seleniumwire\\webdriver.py:179\u001b[0m, in \u001b[0;36mFirefox.__init__\u001b[1;34m(self, seleniumwire_options, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m         capabilities\u001b[38;5;241m.\u001b[39mupdate(config)\n\u001b[0;32m    177\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapabilities\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m capabilities\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\selenium\\webdriver\\firefox\\webdriver.py:197\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, firefox_profile, firefox_binary, capabilities, proxy, executable_path, options, service_log_path, service_args, service, desired_capabilities, log_path, keep_alive)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m    194\u001b[0m executor \u001b[38;5;241m=\u001b[39m FirefoxRemoteConnection(\n\u001b[0;32m    195\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url, ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy\n\u001b[0;32m    196\u001b[0m )\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_remote \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:288\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, command_executor, desired_capabilities, browser_profile, proxy, keep_alive, file_detector, options)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authenticator_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_client()\n\u001b[1;32m--> 288\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcapabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrowser_profile\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:381\u001b[0m, in \u001b[0;36mWebDriver.start_session\u001b[1;34m(self, capabilities, browser_profile)\u001b[0m\n\u001b[0;32m    379\u001b[0m w3c_caps \u001b[38;5;241m=\u001b[39m _make_w3c_caps(capabilities)\n\u001b[0;32m    380\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m: w3c_caps}\n\u001b[1;32m--> 381\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEW_SESSION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m    383\u001b[0m     response \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:442\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    440\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 442\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:294\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    292\u001b[0m data \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdump_json(params)\n\u001b[0;32m    293\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:316\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    313\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 316\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\urllib3\\request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m     75\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\urllib3\\request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(headers)\n\u001b[0;32m    168\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\urllib3\\poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\urllib3\\connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    396\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest_chunked(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 398\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\urllib3\\connection.py:239\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (six\u001b[38;5;241m.\u001b[39mensure_str(k\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m headers):\n\u001b[0;32m    238\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\http\\client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   1254\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\http\\client.py:1302\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\http\\client.py:1251\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1251\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\http\\client.py:1011\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1009\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1011\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1014\u001b[0m \n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\http\\client.py:951\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 951\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\urllib3\\connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[0;32m    183\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\rappi-env\\lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[0;32m     84\u001b[0m         sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m     \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sock\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "be6bf12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 01:28:22,562 - INFO - Inicializando scraper\n",
      "[WDM] - Downloading: 19.2kB [00:00, ?B/s]                                                                              \n",
      "2023-05-08 01:28:30,678 - INFO - Scraper inicializado satisfactoriamente\n"
     ]
    }
   ],
   "source": [
    "scraper = ScraperRappiProducts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b999d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 23:51:43,780 - INFO - Iniciando sesión\n",
      "2023-05-07 23:52:16,592 - INFO - Se inició sesión correctamente\n"
     ]
    }
   ],
   "source": [
    "scraper.login(FB_USERNAME, FB_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "4a939ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper._driver.get(\"https://www.rappi.com.pe/restaurantes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "96bf00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = scraper._wait.until(\n",
    "    lambda x: x.find_elements(\n",
    "        By.XPATH, \"//div[@class='sc-c2b2dc55-4 bkatcD']/a\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "9a6ebfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end=len(restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "d185993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "16\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "index=start\n",
    "while index < end:\n",
    "    try:\n",
    "        scraper._action.move_to_element(restaurants[index]).perform()\n",
    "        index+=1\n",
    "        time.sleep(0.2)\n",
    "    except:\n",
    "        print(index)\n",
    "        scraper._driver.execute_script(f\"window.scrollTo({restaurants[index].location['x']},{restaurants[index].location['y']});\")\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "dbf9a794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Timed out after 3s waiting for request matching 1080-menuspe\n",
      "\n",
      "Message: Timed out after 3s waiting for request matching 31708-la-polleria-del-sindicato\n",
      "\n",
      "Message: Timed out after 3s waiting for request matching 49234-brutal-sandwiches\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for restaurant in restaurants:\n",
    "    restaurant_link = restaurant.get_attribute(\"href\")\n",
    "    # Request que contiene la información del restaurante\n",
    "    try:\n",
    "        request = scraper._driver.wait_for_request(\n",
    "            re.search(\"restaurantes/(.*)\", restaurant_link).group(1),\n",
    "            timeout=3,\n",
    "        )\n",
    "    except Exception as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "d79f9458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 16, 'y': 687}"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants[0].location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "3787b217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d48dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
