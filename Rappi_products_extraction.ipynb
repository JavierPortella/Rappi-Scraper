{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf7df4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías a usar\n",
    "from concurrent import futures as futures\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from traceback import TracebackException\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "from openpyxl import load_workbook, Workbook\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from seleniumwire.utils import decode\n",
    "from seleniumwire.webdriver import Chrome, ChromeOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Constantes\n",
    "CURRENT_DATE = datetime.now().date()\n",
    "ROOT_PATH = os.getcwd()\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "THREAD = futures.ThreadPoolExecutor()\n",
    "ENV = dotenv_values()\n",
    "DATA_FILENAME = ENV(\"DATA_FILENAME\")\n",
    "DATA_FOLDER = ENV(\"DATA_FOLDER\")\n",
    "LOG_FILENAME = ENV(\"LOG_FILENAME\")\n",
    "LOG_FOLDER = ENV(\"LOG_FOLDER\")\n",
    "METADATA_FILENAME = ENV(\"METADATA_FILENAME\")\n",
    "METADATA_SHEET_NAME = ENV(\"METADATA_SHEET_NAME\")\n",
    "FB_USERNAME = ENV(\"FB_USERNAME\")\n",
    "FB_PASSWORD = ENV(\"FB_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a808ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metadata:\n",
    "    \"\"\"Representa a la información generada durante la ejecución del scraper\n",
    "\n",
    "    Attributes:\n",
    "        start_time (float): Hora de inicio de la ejecución del scraper en segundos\n",
    "        end_time (float): Hora de fin de la ejecución del scraper en segundos\n",
    "        execution_date (str): Fecha de extracción de las categorias en formato %d/%m/%Y\n",
    "        start_hour (str): Hora de inicio de la ejecución del scraper en formato %H:%M:%S\n",
    "        end_hour (str): Hora de término de la ejecución del scraper en formato %H:%M:%S\n",
    "        quantity (int): Cantidad de los productos extraídos de la página de Rappi\n",
    "        time_execution (str): Tiempo de ejecución del scraper en segundos\n",
    "        products_per_min (float): Cantidad de categorías que puede extraer el scraper en un minuto\n",
    "        num_errors (int): Cantidad de errores ocurridos durante la ejecución del scraper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Genera todos los atributos para una instancia de la clase Metadata\"\"\"\n",
    "        self._start_time = time.time()\n",
    "        self._end_time = self._start_time\n",
    "        self._execution_date = CURRENT_DATE.strftime(\"%d/%m/%Y\")\n",
    "        self._start_hour = time.strftime(\"%H:%M:%S\", time.localtime(self._start_time))\n",
    "        self._end_hour = self._start_hour\n",
    "        self._quantity = 0\n",
    "        self._time_execution = \"0\"\n",
    "        self._products_per_min = 0\n",
    "        self._num_errors = 0\n",
    "\n",
    "    @property\n",
    "    def num_errors(self):\n",
    "        \"\"\"Retorna el valor actual o actualiza el valor del atributo num_errors\"\"\"\n",
    "        return self._num_errors\n",
    "\n",
    "    @property\n",
    "    def quantity(self):\n",
    "        \"\"\"Retorna el valor actual o actualiza el valor del atributo quantity\"\"\"\n",
    "        return self._quantity\n",
    "\n",
    "    @num_errors.setter\n",
    "    def num_errors(self, num_errors):\n",
    "        self._num_errors = num_errors\n",
    "\n",
    "    @quantity.setter\n",
    "    def quantity(self, quantity):\n",
    "        self._quantity = quantity\n",
    "\n",
    "    def set_attributes_values(self):\n",
    "        \"\"\"Establece los parámetros finales cuando se termina de ejecutar el scraper\"\"\"\n",
    "        self._end_time = time.time()\n",
    "        self._end_hour = time.strftime(\"%H:%M:%S\", time.localtime(self._end_time))\n",
    "        time_execution = self._end_time - self._start_time\n",
    "        if time_execution > 0:\n",
    "            self._time_execution = str(timedelta(seconds=time_execution)).split(\".\")[0]\n",
    "            self._products_per_min = round((self._quantity * 60) / time_execution, 2)\n",
    "\n",
    "    def print_metadata_information(self):\n",
    "        \"\"\"Imprime la información del tiempo de ejecución del scraper por consola\"\"\"\n",
    "        LOGGER.info(f\"Hora inicio: {self._start_hour}\")\n",
    "        LOGGER.info(f\"Hora Fin: {self._end_hour}\")\n",
    "        LOGGER.info(f\"Duración: {self._time_execution}\")\n",
    "        LOGGER.info(f\"Productos Extraídos: {self._quantity}\")\n",
    "        LOGGER.info(f\"Productos Extraídos / min: {self._products_per_min}\")\n",
    "        LOGGER.info(f\"Número de errores: {self._num_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9fc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(TracebackException):\n",
    "    \"\"\"Extiende la clase TracebackException para el manejo del traceback\"\"\"\n",
    "\n",
    "    def __init__(self, error) -> None:\n",
    "        \"\"\"Genera todos los atributos para una instancia de la clase Error\n",
    "\n",
    "        Args:\n",
    "            error (Exception): Error ocurrido durante la ejecución del scraper\n",
    "        \"\"\"\n",
    "        super().__init__(type(error), error, error.__traceback__)\n",
    "\n",
    "    def print_error_detail(self):\n",
    "        \"\"\"Imprime toda la información del error por consola\"\"\"\n",
    "        LOGGER.error(\"Ha ocurrido un error:\")\n",
    "        for line in self.format(chain=True):\n",
    "            LOGGER.error(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperRappiProducts:\n",
    "    \"\"\"Representa a un bot para hacer web scraping en Rappi\n",
    "\n",
    "    Attributes:\n",
    "        metadata (Metadata): Objeto que maneja toda la información generada por el scraper durante su ejecución\n",
    "        products (list): Lista que contiene todos los productos que ofertan los restaurantes en Rappi\n",
    "        dataset (DataFrame): DataFrame que contiene toda la información extraída por el scraper\n",
    "        links_to_go (list): Lista de restaurantes que en un primer intento no se pudo extraer su información\n",
    "        driver (WebDriver): Objeto que maneja el navegador web\n",
    "        wait (WebDriverWait): Objeto que maneja los tiempos de espera de búsqueda de elementos en la web\n",
    "        action (ActionChains): Objeto que maneja las acciones que se pueden aplicar a los elementos en la web\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Genera todos los atributos para una instancia de la clase ScraperRappiProducts\"\"\"\n",
    "        LOGGER.info(\"Inicializando scraper\")\n",
    "        self._metadata = Metadata()\n",
    "        self._products = []\n",
    "        self._restaurants = []\n",
    "        self._dataset = pd.DataFrame()\n",
    "        self._links_to_go = []\n",
    "        chrome_options = ChromeOptions()\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values.notifications\": 2,\n",
    "            \"profile.managed_default_content_settings.popups\": 2,\n",
    "        }\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "        chrome_options.add_experimental_option(\n",
    "            \"excludeSwitches\", [\"enable-logging\"]\n",
    "        )  # Suprimir los mensajes de consola\n",
    "        self._driver = Chrome(\n",
    "            options=chrome_options,\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "        )\n",
    "        self._driver.maximize_window()\n",
    "        self._wait = WebDriverWait(self._driver, 10)\n",
    "        self._action = ActionChains(self._driver)\n",
    "        LOGGER.info(\"Scraper inicializado satisfactoriamente\")\n",
    "\n",
    "    def login(self, user_name, user_password):\n",
    "        \"\"\"Inicia sesión en la página web de Rapi usando una cuenta de Facebook\n",
    "\n",
    "        Args:\n",
    "            user_name (str): Usuario activo de facebook\n",
    "            user_password (str): Contraseña del usuario activo de facebook\n",
    "        \"\"\"\n",
    "        LOGGER.info(\"Iniciando sesión\")\n",
    "        self._driver.get(\"https://www.rappi.com.pe/login\")\n",
    "        # Usar la opción de facebook\n",
    "        self._driver.find_element(\n",
    "            By.XPATH, \"//button[@class='chakra-button css-1hdh3ss']\"\n",
    "        ).click()\n",
    "        time.sleep(random.uniform(3.0, 4.0))\n",
    "        # Cambiar a la pestaña de inicio de sesión de Facebook\n",
    "        self._driver.switch_to.window(self._driver.window_handles[1])\n",
    "        # Completar los campos de usuario y contraseña\n",
    "        username = self._wait.until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "        password = self._wait.until(EC.presence_of_element_located((By.ID, \"pass\")))\n",
    "        username.clear()\n",
    "        password.clear()\n",
    "        username.send_keys(user_name)\n",
    "        password.send_keys(user_password)\n",
    "        # Iniciar sesión\n",
    "        self._driver.find_element(By.CSS_SELECTOR, \"input[name='login']\").click()\n",
    "        # Dando permisos a Rappi en el primer inicio de sesión\n",
    "        try:\n",
    "            self._wait.until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (\n",
    "                        By.XPATH,\n",
    "                        \"//div[@class='x1r8uery x1iyjqo2']\",\n",
    "                    )\n",
    "                )\n",
    "            ).click()\n",
    "        except:\n",
    "            pass\n",
    "        # Volver a la pestaña principal\n",
    "        self._driver.switch_to.window(self._driver.window_handles[0])\n",
    "        time.sleep(random.uniform(8.0, 10.0))\n",
    "        # Detectar si el ícono del usuario aparece en la página\n",
    "        self._wait.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (By.XPATH, \"//div[@class='sc-fdt2fy-11 dJNVzE']\")\n",
    "            )\n",
    "        )\n",
    "        LOGGER.info(\"Se inició sesión correctamente\")\n",
    "\n",
    "    def scrap_product(self, product, rest, status, cat):\n",
    "        \"\"\"Extrae la información de un producto de un restaurante\n",
    "\n",
    "        Args:\n",
    "            product (WebElement): Elemento web que contiene la información del producto de un restaurante\n",
    "            rest (str): Nombre Restaurante\n",
    "            status (str): Estado del restaurante\n",
    "            cat (str): Categoría del restaurante\n",
    "\n",
    "        Returns:\n",
    "            list: Arreglo con toda la información del producto de un restaurante\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        try:\n",
    "            # Popularidad\n",
    "            try:\n",
    "                product.find_element(\n",
    "                    By.XPATH, \".//p[@class='chakra-text css-n0gvg7']\"\n",
    "                ).text\n",
    "                data.append(True)\n",
    "            except:\n",
    "                data.append(False)\n",
    "            # Nombre\n",
    "            data.append(\n",
    "                product.find_element(By.XPATH, \".//div[@class='css-k008qs']\").text\n",
    "            )\n",
    "            # Descripción\n",
    "            data.append(\n",
    "                product.find_element(\n",
    "                    By.XPATH,\n",
    "                    \".//p[@class='chakra-text sc-a04fe063-2 gHQcCO css-1rmjo0r']\",\n",
    "                ).text\n",
    "            )\n",
    "            # Precios\n",
    "            prices = product.find_element(\n",
    "                By.XPATH, \".//.//div[contains(@class, 'chakra-skeleton')]\"\n",
    "            ).text.split(\"S/ \")\n",
    "            # Precio con descuento\n",
    "            try:\n",
    "                data.append(prices[-2])\n",
    "            except:\n",
    "                data.append(None)\n",
    "            # Precio sin descuento\n",
    "            data.append(prices[-1])\n",
    "            # Nombre del restaurante\n",
    "            data.append(rest)\n",
    "            # Disponibilidad\n",
    "            data.append(status)\n",
    "            # Categoría\n",
    "            data.append(cat)\n",
    "            return data\n",
    "        except Exception as error:\n",
    "            try:\n",
    "                LOGGER.error(f\"Error al scrapear el producto {data[1]}\")\n",
    "            except:\n",
    "                pass\n",
    "            LOGGER.error(f\"Error de tipo: {error.__class__}\")\n",
    "            return []\n",
    "\n",
    "    def scrap_restaurante(self, rest, link, status):\n",
    "        \"\"\"Ingresa a un restaurante y extrae la información de todos los productos que ofrece\n",
    "\n",
    "        Args:\n",
    "            rest (str): Nombre del restaurante\n",
    "            link (str): Link del restaurante\n",
    "            status (str): Estado actual del restaurante\n",
    "        \"\"\"\n",
    "        try:\n",
    "            LOGGER.info(f\"Extrayendo información del restaurante {rest}\")\n",
    "            self._driver.get(link)\n",
    "            # Productos que ofrece el restaurante\n",
    "            products = self._driver.find_elements(\n",
    "                By.XPATH, '//div[@class=\"chakra-stack css-46p1lt\"]'\n",
    "            )\n",
    "            # Categoría del restaurante\n",
    "            try:\n",
    "                category = self._driver.find_element(\n",
    "                    By.XPATH, \"//div[@class='sc-3627ee44-1 dEYlRK']/h2[2]\"\n",
    "                ).text\n",
    "            except:\n",
    "                LOGGER.info(f\"El restaurante no posee ninguna categoría\")\n",
    "                category = None\n",
    "            # Extrayendo la información de varios productos a la vez\n",
    "            future_products = [\n",
    "                THREAD.submit(self.scrap_product, product, rest, status, category)\n",
    "                for product in products\n",
    "            ]\n",
    "            for future_product in futures.wait(future_products).done:\n",
    "                self._products.append(future_product.result())\n",
    "        except Exception as error:\n",
    "            LOGGER.error(\"Fallo al extraer la información de los productos\")\n",
    "            LOGGER.error(f\"Error de tipo: {error.__class__}\")\n",
    "\n",
    "    def extract_products(self, restaurant):\n",
    "        \"\"\"Extrae la información de los productos de un restaurante usando los recursos de red\n",
    "\n",
    "        Args:\n",
    "            restaurant (WebElement): Elemento web que representa a un restaurante\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._action.scroll_to_element(restaurant).move_to_element(\n",
    "                restaurant\n",
    "            ).perform()\n",
    "            # Disponibilidad del restaurante\n",
    "            try:\n",
    "                restaurant_status = restaurant.find_element(\n",
    "                    By.XPATH, \".//p[contains(@class,'chakra-text')]\"\n",
    "                ).text\n",
    "            except:\n",
    "                restaurant_status = \"Restaurante abierto\"\n",
    "            # Enlace web del restaurante\n",
    "            restaurant_link = restaurant.get_attribute(\"href\")\n",
    "            self._restaurants.append(restaurant_link)\n",
    "            # Request que contiene la información del restaurante\n",
    "            request = self._driver.wait_for_request(\n",
    "                re.search(\"restaurantes/(.*)\", restaurant_link).group(1),\n",
    "                timeout=3,\n",
    "            )\n",
    "            # Obteniendo la respuesta decodificada del request\n",
    "            decoded_body = decode(\n",
    "                request.response.body,\n",
    "                request.response.headers.get(\"Content-Encoding\", \"identity\"),\n",
    "            ).decode(\"utf-8\")\n",
    "            # Convirtiendo en formato json\n",
    "            json_data = json.loads(decoded_body)\n",
    "            rest_dict = json_data[\"pageProps\"][\"fallback\"]\n",
    "            restaurant_data = rest_dict[next(iter(rest_dict))]\n",
    "            # Nombre y categoría del restaurante\n",
    "            restaurant_name = restaurant_data.get(\"brandName\")\n",
    "            restaurant_category = restaurant_data.get(\"categories\")\n",
    "            return [\n",
    "                [\n",
    "                    product.get(\"isPopular\", False),\n",
    "                    product.get(\"name\"),\n",
    "                    product.get(\"description\"),\n",
    "                    product.get(\"priceNumber\"),\n",
    "                    product.get(\"realPrice\"),\n",
    "                    restaurant_name,\n",
    "                    restaurant_status,\n",
    "                    restaurant_category,\n",
    "                ]\n",
    "                for product_category in restaurant_data[\"corridors\"]\n",
    "                for product in product_category[\"products\"]\n",
    "            ]\n",
    "        except Exception as error:\n",
    "            self._metadata.num_errors += 1\n",
    "            rest_name = restaurant.get_attribute(\"aria-label\")\n",
    "            LOGGER.error(\n",
    "                f\"Fallo al intentar extraer la información de los productos ofrecidos por el restaurante {rest_name}\"\n",
    "            )\n",
    "            LOGGER.error(error)\n",
    "            self._links_to_go.append((rest_name, restaurant_link, restaurant_status))\n",
    "            return []\n",
    "\n",
    "    def extract_data(self):\n",
    "        \"\"\"Extrae todos los productos que ofertan los restaurantes en Rappi\"\"\"\n",
    "        LOGGER.info(\"Extrayendo los productos de rappi\")\n",
    "        self._driver.get(\"https://www.rappi.com.pe/restaurantes\")\n",
    "        del self._driver.requests\n",
    "\n",
    "        # Extracción de información de los restaurantes más cercanos\n",
    "        no_error = True\n",
    "        start = 0\n",
    "        while no_error:\n",
    "            # Identificando el botón de ver más restaurantes\n",
    "            try:\n",
    "                button = self._wait.until(\n",
    "                    EC.element_to_be_clickable(\n",
    "                        (\n",
    "                            By.XPATH,\n",
    "                            \"//button[@class='sc-hqyNC sc-jbKcbu bvSdOe primary wide']\",\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            except Exception as error:\n",
    "                no_error = False\n",
    "\n",
    "            # Esperando a que se carguen los restaurantes\n",
    "            time.sleep(random.uniform(2.0, 3.0))\n",
    "            restaurants = self._wait.until(\n",
    "                lambda x: x.find_elements(\n",
    "                    By.XPATH,\n",
    "                    \"//div[@class='sc-c2b2dc55-4 bkatcD']/a\",\n",
    "                )\n",
    "            )\n",
    "            end = len(restaurants)\n",
    "            # Interactuando con varios restaurantes a la vez\n",
    "            future_restaurants = [\n",
    "                THREAD.submit(self.extract_products, restaurants[index])\n",
    "                for index in range(start, end)\n",
    "            ]\n",
    "            for future_restaurant in futures.wait(future_restaurants).done:\n",
    "                self._products += future_restaurant.result()\n",
    "            LOGGER.info(f\"Cantidad de restaurantes totales recorridos: {end}\")\n",
    "            # Dar click al botón de ver más restaurantes\n",
    "            try:\n",
    "                del self._driver.requests\n",
    "                button.click()\n",
    "                start = end\n",
    "            except:\n",
    "                no_error = False\n",
    "\n",
    "        LOGGER.info(\n",
    "            f\"Se extrajo la información de {len(self._restaurants)} restaurante(s)\"\n",
    "        )\n",
    "\n",
    "        # Extracción de información de los restaurantes por categorías\n",
    "        self._driver.get(\"https://www.rappi.com.pe/restaurantes\")\n",
    "        del self._driver.requests\n",
    "        # Categorías de los restaurantes\n",
    "        categories = self._driver.find_elements(\n",
    "            By.XPATH, \"//button[@class='sc-5d042f5c-1 iyWZJm']\"\n",
    "        )\n",
    "        LOGGER.info(\n",
    "            f\"Se han detectado {len(categories)} tipos de restaurantes según su categoría\"\n",
    "        )\n",
    "        for category in categories:\n",
    "            LOGGER.info(f\"Categoría: {category.text}\")\n",
    "            # Navegar a la parte superior de la página\n",
    "            self._driver.execute_script(\n",
    "                \"window.scrollTo(document.body.scrollHeight, 0)\"\n",
    "            )\n",
    "            time.sleep(random.uniform(1.0, 1.5))\n",
    "            # Dar click a una categoría\n",
    "            try:\n",
    "                category.click()\n",
    "                time.sleep(random.uniform(7.0, 8.0))\n",
    "            except Exception as error:\n",
    "                LOGGER.error(\"No se ha podido dar click a la categoría\")\n",
    "                LOGGER.error(f\"Error de tipo {error.__class__}\")\n",
    "                continue\n",
    "            # Dar click a la flecha de navegación de las categorías\n",
    "            try:\n",
    "                self._driver.find_element(By.CLASS_NAME, \"sc-69ee8a42-2\").click()\n",
    "            except:\n",
    "                pass\n",
    "            # Identificar si la categoría cuenta con restaurantes\n",
    "            try:\n",
    "                # Identificar los restaurantes pertenecientes a la categoría seleccionada\n",
    "                restaurants = self._wait.until(\n",
    "                    lambda x: x.find_elements(\n",
    "                        By.XPATH, \"//div[@class='sc-c2b2dc55-4 bkatcD']/a\"\n",
    "                    )\n",
    "                )\n",
    "                # Filtrar los restaurantes con las que ya se cuente información\n",
    "                restaurants = [\n",
    "                    restaurant\n",
    "                    for restaurant in restaurants\n",
    "                    if restaurant.get_attribute(\"href\") not in self._restaurants\n",
    "                ]\n",
    "                del self._driver.requests\n",
    "                LOGGER.info(\n",
    "                    f\"Se va a extraer información de los productos de {len(restaurants)} restaurante(s)\"\n",
    "                )\n",
    "            except:\n",
    "                LOGGER.info(\"La categoría no cuenta con restaurantes\")\n",
    "                continue\n",
    "            # Extraer la información de varios restaurantes a la vez\n",
    "            future_restaurants = [\n",
    "                THREAD.submit(self.extract_products, restaurant)\n",
    "                for restaurant in restaurants\n",
    "            ]\n",
    "            for future_restaurant in futures.wait(future_restaurants).done:\n",
    "                self._products += future_restaurant.result()\n",
    "            del self._driver.requests\n",
    "\n",
    "        # Eliminar valores duplicados:\n",
    "        self._links_to_go = list(set(self._links_to_go))\n",
    "        LOGGER.info(f\"Se van a recorrer {len(self._links_to_go)} restaurantes\")\n",
    "        # Extrayendo la información de los restaurantes faltantes\n",
    "        for rest_name, restaurant_link, restaurant_status in self._links_to_go:\n",
    "            del self._driver.requests\n",
    "            self.scrap_restaurante(rest_name, restaurant_link, restaurant_status)\n",
    "        LOGGER.info(\"Extracción de datos completada satisfactoriamente\")\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"Proceso de limpieza de datos extraídos por el scraper\"\"\"\n",
    "        try:\n",
    "            LOGGER.info(\"Limpiando la data extraída por el scraper\")\n",
    "            self._dataset = pd.DataFrame(\n",
    "                self._products,\n",
    "                columns=[\n",
    "                    \"Popular\",\n",
    "                    \"Producto\",\n",
    "                    \"Descripcion\",\n",
    "                    \"Precio con descuento\",\n",
    "                    \"Precio sin descuento\",\n",
    "                    \"Restaurante\",\n",
    "                    \"Disponible\",\n",
    "                    \"Categoria\",\n",
    "                ],\n",
    "            )\n",
    "            self._products = []\n",
    "            self._dataset[\"Fecha\"] = CURRENT_DATE.strftime(\"%Y-%m-%d\")\n",
    "            self._dataset.sort_values(\n",
    "                [\"Restaurante\", \"Producto\", \"Descripcion\", \"Popular\"],\n",
    "                inplace=True,\n",
    "                ascending=[True, True, True, False],\n",
    "            )\n",
    "            self._dataset.drop_duplicates(\n",
    "                [\"Restaurante\", \"Producto\", \"Descripcion\"], keep=\"first\", inplace=True\n",
    "            )\n",
    "            self._dataset.replace({\"\": None}, inplace=True)\n",
    "            self._dataset = self._dataset.astype(\n",
    "                {\n",
    "                    \"Precio con descuento\": float,\n",
    "                    \"Precio sin descuento\": float,\n",
    "                    \"Popular\": str,\n",
    "                }\n",
    "            )\n",
    "            self._dataset[\"Precio con descuento\"] = self._dataset[\n",
    "                \"Precio con descuento\"\n",
    "            ].apply(lambda x: round(x, 2))\n",
    "            self._dataset.loc[\n",
    "                self._dataset[\n",
    "                    self._dataset[\"Precio con descuento\"]\n",
    "                    == self._dataset[\"Precio sin descuento\"]\n",
    "                ].index,\n",
    "                \"Precio con descuento\",\n",
    "            ] = None\n",
    "            self._dataset[\n",
    "                [\"Precio con descuento\", \"Precio sin descuento\"]\n",
    "            ] = self._dataset[\n",
    "                [\"Precio con descuento\", \"Precio sin descuento\"]\n",
    "            ].applymap(\n",
    "                \"{:,.2f}\".format\n",
    "            )\n",
    "            self._dataset[[\"Precio con descuento\", \"Precio sin descuento\"]] = (\n",
    "                self._dataset[[\"Precio con descuento\", \"Precio sin descuento\"]]\n",
    "                .replace({\",\": \";\", \"\\.\": \",\"}, regex=True)\n",
    "                .replace({\";\": \".\"}, regex=True)\n",
    "            )\n",
    "            self._dataset[\"Disponible\"].replace(\n",
    "                \"^Abre.+\", \"Restaurante cerrado\", regex=True, inplace=True\n",
    "            )\n",
    "            self._dataset[\"Categoria\"].replace(\" -.+\", \"\", regex=True, inplace=True)\n",
    "            self._dataset[\"Restaurante\"].replace(\" -.+\", \"\", regex=True, inplace=True)\n",
    "            self._dataset[\"Popular\"].replace(\n",
    "                {\"True\": \"popular\", \"False\": \"\"}, inplace=True\n",
    "            )\n",
    "            LOGGER.info(\"Se ha limpiado la data satisfactoriamente\")\n",
    "        except Exception as error:\n",
    "            LOGGER.error(\"Error al ejecutar el proceso completo de limpieza de datos\")\n",
    "            LOGGER.error(error)\n",
    "\n",
    "    def save_data(self, filepath, filename, encoding=\"utf-8-sig\"):\n",
    "        \"\"\"Guarda los datos o errores obtenidos durante la ejecución del scraper\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Ruta del archivo\n",
    "            filename (str): Nombre del archivo\n",
    "            encoding (str): Codificación usada para guardar el archivo. Defaults to \"utf-8-sig\"\n",
    "        \"\"\"\n",
    "        LOGGER.info(\"Guardando la data\")\n",
    "        # Comprobando que el dataset contenga información\n",
    "        if len(self._dataset) == 0:\n",
    "            LOGGER.info(\n",
    "                f\"El archivo de datos no se va a guardar por no tener información\",\n",
    "            )\n",
    "            return\n",
    "\n",
    "        self._metadata.quantity = len(self._dataset)\n",
    "        # Generando la ruta donde se va a guardar la información\n",
    "        filepath = os.path.join(filepath, CURRENT_DATE.strftime(\"%d-%m-%Y\"))\n",
    "        filename = (\n",
    "            filename\n",
    "            + \"_\"\n",
    "            + CURRENT_DATE.strftime(\"%Y-%m-%d\")\n",
    "            + \"_\"\n",
    "            + str(self._metadata.quantity)\n",
    "            + \".csv\"\n",
    "        )\n",
    "\n",
    "        # Verificando si la ruta donde se va a guardar la información existe\n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "\n",
    "        self._dataset.to_csv(\n",
    "            os.path.join(filepath, filename),\n",
    "            sep=\";\",\n",
    "            index=False,\n",
    "            encoding=encoding,\n",
    "        )\n",
    "        LOGGER.info(\n",
    "            f\"El archivo de datos {filename} ha sido guardado correctamente en la ruta {os.path.join(ROOT_PATH, filepath)}\",\n",
    "        )\n",
    "\n",
    "    def save_metadata(self, filename, sheet_name):\n",
    "        \"\"\"Guarda la información de la metadata generada durante la ejecución del scraper\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo\n",
    "            sheet_name (str): Nombre de la hoja de cálculo\n",
    "        \"\"\"\n",
    "        LOGGER.info(\"Guardando la metadata\")\n",
    "        self._metadata.set_attributes_values()\n",
    "        self._metadata.print_metadata_information()\n",
    "        # Variable que indica si el encabezado existe o no en el archivo de excel\n",
    "        header_exist = False\n",
    "\n",
    "        # Verificando si el archivo existe o no\n",
    "        if os.path.isfile(filename):\n",
    "            wb_time = load_workbook(filename)\n",
    "            # Comprobando si ya existe un sheet con el nombre indicado en la variable sheet_name\n",
    "            if sheet_name not in [ws.title for ws in wb_time.worksheets]:\n",
    "                # Creando un nuevo sheet\n",
    "                wb_time.create_sheet(sheet_name)\n",
    "            else:\n",
    "                header_exist = True\n",
    "        else:\n",
    "            wb_time = Workbook()\n",
    "            wb_time.worksheets[0].title = sheet_name\n",
    "\n",
    "        # Seleccionar el sheet deseado donde se va a guardar la información\n",
    "        worksheet = wb_time[sheet_name]\n",
    "\n",
    "        # Comprobando si el encabezado existe o no\n",
    "        if not header_exist:\n",
    "            keys = [\n",
    "                \"Fecha\",\n",
    "                \"Hora Inicio\",\n",
    "                \"Hora Fin\",\n",
    "                \"Cantidad\",\n",
    "                \"Tiempo Ejecucion (min)\",\n",
    "                \"Productos / Minuto\",\n",
    "                \"Errores\",\n",
    "            ]\n",
    "            worksheet.append(keys)\n",
    "\n",
    "        values = list(self._metadata.__dict__.values())[2:]\n",
    "        worksheet.append(values)\n",
    "        wb_time.save(filename)\n",
    "        wb_time.close()\n",
    "        LOGGER.info(\n",
    "            f\"El archivo de la metadata del scraper {filename} ha sido guardado correctamente en la ruta {ROOT_PATH}\",\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Ejecuta el proceso completo de web scraping a Rappi\"\"\"\n",
    "        self.login(FB_USERNAME, FB_PASSWORD)\n",
    "        self.extract_data()\n",
    "        self.process_data()\n",
    "        self.save_data(DATA_FOLDER, DATA_FILENAME)\n",
    "        self.save_metadata(METADATA_FILENAME, METADATA_SHEET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c70c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_log(\n",
    "    log_folder, log_filename, log_file_mode=\"w\", log_file_encoding=\"utf-8\"\n",
    "):\n",
    "    \"\"\"Función que configura los logs para rastrear al programa\n",
    "\n",
    "    Args:\n",
    "        log_folder (str): Carpeta donde se va a generar el archivo log\n",
    "        log_filename (str): Nombre del archivo log a ser generado\n",
    "        log_file_mode (str, optional): Modo de guardado del archivo. Defaults to \"w\".\n",
    "        log_file_encoding (str, optional): Codificación usada para el archivo. Defaults to \"utf-8\".\n",
    "    \"\"\"\n",
    "    # Generando la ruta donde se va a guardar los registros de ejecución\n",
    "    log_path = os.path.join(log_folder, CURRENT_DATE.strftime(\"%d-%m-%Y\"))\n",
    "    log_filename = log_filename + \"_\" + CURRENT_DATE.strftime(\"%d%m%Y\") + \".log\"\n",
    "\n",
    "    # Verificando si la ruta donde se va a guardar los registros de ejecución existe\n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path)\n",
    "\n",
    "    # Agregando los handlers al logger\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    file_handler = logging.FileHandler(\n",
    "        os.path.join(log_path, log_filename), log_file_mode, log_file_encoding\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "    LOGGER.handlers = [stream_handler, file_handler]\n",
    "    LOGGER.propagate = False\n",
    "    LOGGER.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4343142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        configure_log(LOG_FOLDER, LOG_FILENAME)\n",
    "        scraper = ScraperRappiProducts()\n",
    "        scraper.run()\n",
    "        LOGGER.info(\"Programa finalizado\")\n",
    "    except Exception as error:\n",
    "        Error(error).print_error_detail()\n",
    "        LOGGER.error(\"Programa ejecutado con fallos\")\n",
    "    finally:\n",
    "        # Liberar el archivo log\n",
    "        logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
